{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f95834a6",
   "metadata": {},
   "source": [
    "# GET your API from Here \n",
    "- https://aistudio.google.com/app/apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bbc86ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='S. DINESH KUMAR || 2021PECAI250 \\nsvani4830@gmail.com ||+91 9342372124 ||https://github.com/SDineshKumar1304 || www.linkedin.com/in/s-dineshkumar2004  \\nPROFESSIONAL EXPERIENCE \\nINNOMATICS RESEARCH LABS                                                                                                India, Hyderabad \\nJunior Data Scientist Intern                                                                                             Jan 2024-Present (Remote) \\nML Flow Model Experimentation: \\nApplication Development: \\nExploratory Data Analysis: \\n● Developed ” Gen-AI A Multi-faceted AI Endeavor ”.Working on Gemini-1.5 pro  LLM  \\n● Utilized Chroma DB  to develop a Search Engine  using tfidf-vectorizer and Cosine similarity  model for \\nMovie Subtitles information retrieval system. \\n● Developed Real Time Sentiment Analysis on Flipkart Products reviews web application with Flask \\nDeployed the Web application on AWS \\n● Utilized ML Flow and Prefect Orchestration for model Experimentation and Model Management.  \\n● Real-time Intern Demographic Analysis: Analysed and visualized intern demographics in real-time.Data \\nRecovery and Misconduct Identification using python. \\n● Data-Driven Insights for Pizza Store Operations: Improved delivery performance and revenue management \\nthrough data analysis . \\n● EDA of AMCAT  Dataset and Hypothesis Testing: Investigated salary claims using exploratory data analysis \\nand hypothesis testing.  \\n● Regex Matching Web App Development and AWS Deployment: Created and deployed a regex matching web \\napplication on AWS  \\nRadicalX AI New York, NY \\nAI Engineer Intern NOV 2023 – Jan 2024 (Remote) \\n\\uf0b7 Constructed a Large Language Model for Prompt Parsing utilizing the GPT TURBO-3.5.  \\n\\uf0b7 Evaluated LLM responses where there isn\\'t a single \"right answer\".  \\nZigson Technologies Pvt Ltd I NDIA \\nData Analyst Intern Dec 2023 – Dec 2023 (Remote) \\n\\uf0b7 Designed a user-friendly dashboard using Power BI to analyze Global Superstore sales data.  \\n\\uf0b7 Enhanced data accuracy by building strong relationships between Attributes.  \\nTechnohacks Edutech Pvt Ltd I NDIA \\nDATA SCIENCE INTERN  Nov 2023 – Dec 2023 (Remote) \\n● Trained RandomForestClassifier with Accuracy:81.05% on Telco Customer Churn Analysis . \\n● Created an interactive real-time Analysis Web dashboard to gauge the Analysis in real time \\nEDUCATION \\nPANIMALAR ENGINEERING COLLEGE I NDIA, TN \\nBachelor of Technology in Artificial Intelligence and Data Science - grade-8.4(upto 5th Sem) 2021-2025 \\nScholastic Achievements \\n● Copyrights Certified for my 1st Software Work Idea on the Domain of Artificial Intelligence. \\n● Secured 2nd Copyrights Groundbreaking HealthCare Solution in the domain of Artificial intelligence. \\n● Hacker Rank Skill Certifications and Obtained Gold Badge for Basic of Python. \\n● Hacker Rank Skill Certification and Obtained Silver Badge for the Basics of SQL. \\n● Hacker Rank Skill Certification for Software Engineer Intern Role . \\n● Udemy Certification for Numpy, Pandas, Matplot, Scipy and Machine Learning. \\n● Attained Gold Category with 81% in NASSCOM Certification for Data Science for Beginners. \\n● Microsoft and LinkedIn\\'s Generative AI program, earned certification in Career Essentials. \\n● Secured Global Rank 164 in IEEE Extreme 17.0 coding Competition IEEE Excellence Award.  \\n● Secured Top 10% in 6-hour Hackathon by Innomatics Research Labs \\n● Excelled among 20,000+ applicants and Shortlisted for Data Science Internship Opportunity. \\nVAILANKANNI MATRIC HIGHER SECONDARY SCHOOL  INDIA,  TN \\nHigher Secondary Major in Mathematics and Biology  2020-2021 \\n● Attained 93% of Class XII, TamilNadu Board \\nVAILANKANNI MATRIC HIGHER SECONDARY SCHOOL  INDIA,  TN \\nSSLC, Major in Mathematics and Biology  2018-2019 \\n● Attained 93.3% of Class X, TamilNadu Board', metadata={'source': 'Resume.pdf', 'page': 0}),\n",
       " Document(page_content='ADDITIONAL INFORMATION \\n● Technical Skills: Python,Flask,Streamlit,SQL&DBMS,EDA,MLOPS,Machine Learning,Data \\nAnalytics,PowerBI, NLP, AWS Deployment. \\n● Languages: Fluent in Tamil (native), English, Telugu. \\nRESEARCH ENDEAVORS \\n \\nNLP \\n\\uf0b7 Created a special voice assistant that understands how users feel when they speak.  \\n\\uf0b7 Developed a special way to give personalized advice based on how the user feels.  \\n\\uf0b7 Stands out as a friendly and supportive assistant, offering a special experience unlike others.  \\nWeb Application (Mlops) \\n\\uf0b7 Collected and parsed data manually for sentiment analysis.  \\n\\uf0b7 Built Natural Language Processing Sentiment analysis model with Accuracy: 93.41%.  \\n\\uf0b7 Constructed pipeline, TF-IDF vectorization and a SVM with linear kernel for model training.  \\n\\uf0b7 Integrated the model into a Flask server for real-time predictions.  \\n\\uf0b7 Designed and implemented a visually appealing web interface using basic HTML and CSS.  \\n\\uf0b7 Implemented user registration functionality using an SQL database within the Flask application.  \\n\\uf0b7 Conducted data recollection and visualized results in Excel.  \\nWeb Application (Thirukkural Bot) \\n\\uf0b7 Involved developing Thirukkural Web application for quick detail Extraction for each Kural.  \\n\\uf0b7 Utilized Streamlit for UI Development  \\n\\uf0b7 Successfully tested the application prototype functions and results.  \\nPUBLICATIONS \\nGCITC Conference IEEEExplore (2024) (Published) \\n\\uf0b7 Presented Groundbreaking Research Paper in REVA University.  \\n\\uf0b7 Utilized advanced technology, including Mask R-CNN, Marching Cube Algorithm, voxelization and computer \\nvision technique.  \\nPEC TEAM Conference (2024 ) (Yet to be Published) \\n\\uf0b7 Contributed Research on Decision Tree Classifier for Healthcare Resource Allocation System.  \\n    PEC TEAM Conference (2023) (Yet to be Published) \\n\\uf0b7 Explored research on Trilateration Algorithm for Lost Object Tracking  \\nSOCIAL IMPACT \\n\\uf0b7 Dedicated my Commitment to Social Responsibility and to make Positive Community.  \\n\\uf0b7 Collaborated with Local Community, impacted over 70+kids by providing Supportive education and Preventive \\nEmpowerment.  \\nEXTRA-CURRICULAR ACTIVITIES \\nSports   - Cricket, chess, Carrom \\nHobbies - Playing Mobile games, listening music,Drawing .', metadata={'source': 'Resume.pdf', 'page': 1})]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(r\"Resume.pdf\")\n",
    "data = loader.load_and_split()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d0a7ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 784, which is longer than the specified 200\n",
      "Created a chunk of size 227, which is longer than the specified 200\n",
      "Created a chunk of size 280, which is longer than the specified 200\n",
      "Created a chunk of size 370, which is longer than the specified 200\n",
      "Created a chunk of size 221, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=200, chunk_overlap=200)\n",
    "\n",
    "chunks = text_splitter.split_documents(data)\n",
    "\n",
    "print(len(chunks))\n",
    "\n",
    "print(type(chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93471b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(google_api_key=\"YOUR_API_KEY\", \n",
    "                                               model=\"models/embedding-001\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a3b60f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Downloading chromadb-0.5.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from chromadb) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from chromadb) (2.3.0)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-win_amd64.whl.metadata (262 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Downloading fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.29.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from chromadb) (1.25.2)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from chromadb) (4.7.1)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.17.3-cp310-cp310-win_amd64.whl.metadata (4.6 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from chromadb) (0.15.2)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "     ---------------------------------------- 0.0/67.3 kB ? eta -:--:--\n",
      "     ------------------ --------------------- 30.7/67.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.3/67.3 kB 1.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from chromadb) (4.66.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from chromadb) (1.63.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.1.2-cp39-abi3-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from chromadb) (0.9.0)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from chromadb) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from chromadb) (6.0.1)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-4.1.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from chromadb) (3.10.2)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting typing-extensions>=4.5.0 (from chromadb)\n",
      "  Using cached typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading fastapi_cli-0.0.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx>=0.23.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (3.1.2)\n",
      "Collecting python-multipart>=0.0.7 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading ujson-5.9.0-cp310-cp310-win_amd64.whl.metadata (8.9 kB)\n",
      "Collecting email_validator>=2.0.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading email_validator-2.1.1-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2023.7.22)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.22.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.58.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.26.16)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: protobuf in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: importlib-metadata<=7.0,>=6.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (68.2.2)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests>=2.28->chromadb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests>=2.28->chromadb) (2.10)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.22.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.9.0)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.1-cp310-cp310-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-0.21.0-cp310-none-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (10.4)\n",
      "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb)\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from typer>=0.9.0->chromadb) (13.7.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.7.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (3.5.0)\n",
      "Collecting httpcore==1.* (from httpx>=0.23.0->fastapi>=0.95.2->chromadb)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (1.2.0)\n",
      "Collecting h11>=0.8 (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.9.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.4.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Downloading chromadb-0.5.0-py3-none-any.whl (526 kB)\n",
      "   ---------------------------------------- 0.0/526.8 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 71.7/526.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 153.6/526.8 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 215.0/526.8 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 297.0/526.8 kB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 368.6/526.8 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 450.6/526.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  522.2/526.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 526.8/526.8 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading chroma_hnswlib-0.7.3-cp310-cp310-win_amd64.whl (150 kB)\n",
      "   ---------------------------------------- 0.0/150.6 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 81.9/150.6 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 150.6/150.6 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading bcrypt-4.1.2-cp39-abi3-win_amd64.whl (158 kB)\n",
      "   ---------------------------------------- 0.0/158.3 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 81.9/158.3 kB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 158.3/158.3 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading build-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
      "   ---------------------------------------- 0.0/92.0 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 81.9/92.0 kB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 92.0/92.0 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.6 MB 6.8 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.2/1.6 MB 3.0 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.3/1.6 MB 2.9 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.4/1.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.5/1.6 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.6/1.6 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.7/1.6 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.7/1.6 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.6 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.9/1.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.0/1.6 MB 2.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.1/1.6 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.2/1.6 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.3/1.6 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.4/1.6 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.5/1.6 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 2.4 MB/s eta 0:00:00\n",
      "Downloading mmh3-4.1.0-cp310-cp310-win_amd64.whl (31 kB)\n",
      "Downloading onnxruntime-1.17.3-cp310-cp310-win_amd64.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/5.6 MB 3.0 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.2/5.6 MB 3.1 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.3/5.6 MB 2.5 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.4/5.6 MB 2.9 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.5/5.6 MB 2.7 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.6/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.7/5.6 MB 2.7 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.8/5.6 MB 2.7 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.8/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.9/5.6 MB 2.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.0/5.6 MB 2.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.1/5.6 MB 2.5 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.2/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.3/5.6 MB 2.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.4/5.6 MB 2.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.5/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.7/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.8/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 1.9/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.0/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.1/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.2/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.3/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.4/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.5/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.6/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 2.7/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 2.8/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 2.9/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.0/5.6 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.1/5.6 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.2/5.6 MB 2.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.3/5.6 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.4/5.6 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.5/5.6 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.6/5.6 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.7/5.6 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.9/5.6 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.0/5.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.1/5.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.2/5.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.3/5.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.4/5.6 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.5/5.6 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.6/5.6 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.7/5.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.8/5.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.0/5.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.1/5.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.2/5.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.3/5.6 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.4/5.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.6/5.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.6/5.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.1/60.1 kB ? eta 0:00:00\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.8/50.8 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
      "Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
      "Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
      "   ---------------------------------------- 0.0/106.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 106.1/106.1 kB 6.0 MB/s eta 0:00:00\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
      "   ---------------------------------------- 0.0/41.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 41.3/41.3 kB 1.9 MB/s eta 0:00:00\n",
      "Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
      "Downloading fastapi_cli-0.0.2-py3-none-any.whl (9.1 kB)\n",
      "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.2/47.2 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading httptools-0.6.1-cp310-cp310-win_amd64.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.2/58.2 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 0.0/75.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 75.6/75.6 kB 4.1 MB/s eta 0:00:00\n",
      "Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "   ---------------------------------------- 0.0/77.9 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 71.7/77.9 kB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 77.9/77.9 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.9/71.9 kB 4.1 MB/s eta 0:00:00\n",
      "Downloading ujson-5.9.0-cp310-cp310-win_amd64.whl (41 kB)\n",
      "   ---------------------------------------- 0.0/41.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 41.9/41.9 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading watchfiles-0.21.0-cp310-none-win_amd64.whl (279 kB)\n",
      "   ---------------------------------------- 0.0/279.7 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 133.1/279.7 kB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 266.2/279.7 kB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 279.7/279.7 kB 2.9 MB/s eta 0:00:00\n",
      "Downloading pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "   ---------------------------------------- 0.0/307.7 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 122.9/307.7 kB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 266.2/307.7 kB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 307.7/307.7 kB 3.2 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53835 sha256=dbfc5020e015ddce8f7f9b11c22234d5b7a560eb2951248ef1c19003afbd3301\n",
      "  Stored in directory: c:\\users\\svani\\appdata\\local\\pip\\cache\\wheels\\e1\\26\\51\\d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, mmh3, ujson, typing-extensions, shellingham, python-multipart, pyproject_hooks, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, importlib-resources, httptools, h11, dnspython, deprecated, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, httpcore, email_validator, build, asgiref, typer, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, httpx, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, fastapi-cli, fastapi, chromadb\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.9.0\n",
      "    Uninstalling h11-0.9.0:\n",
      "      Successfully uninstalled h11-0.9.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 0.9.1\n",
      "    Uninstalling httpcore-0.9.1:\n",
      "      Successfully uninstalled httpcore-0.9.1\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.9.0\n",
      "    Uninstalling typer-0.9.0:\n",
      "      Successfully uninstalled typer-0.9.0\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.13.3\n",
      "    Uninstalling httpx-0.13.3:\n",
      "      Successfully uninstalled httpx-0.13.3\n",
      "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.2 build-1.2.1 chroma-hnswlib-0.7.3 chromadb-0.5.0 deprecated-1.2.14 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.2 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 importlib-resources-6.4.0 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.17.3 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 opentelemetry-util-http-0.45b0 overrides-7.7.0 posthog-3.5.0 pypika-0.48.9 pyproject_hooks-1.1.0 python-multipart-0.0.9 shellingham-1.5.4 starlette-0.37.2 typer-0.12.3 typing-extensions-4.11.0 ujson-5.9.0 uvicorn-0.29.0 watchfiles-0.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "googletrans 4.0.0rc1 requires httpx==0.13.3, but you have httpx 0.27.0 which is incompatible.\n",
      "langchain 0.0.340 requires langsmith<0.1.0,>=0.0.63, but you have langsmith 0.1.53 which is incompatible.\n",
      "spacy 3.6.1 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c587535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svani\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Embed each chunk and load it into the vector store\n",
    "db = Chroma.from_documents(chunks, embedding_model, persist_directory=\"./chroma_db_\")\n",
    "\n",
    "# Persist the database on drive\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7248f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connection = Chroma(persist_directory=\"./chroma_db_\", embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9cf2fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.vectorstores.VectorStoreRetriever'>\n"
     ]
    }
   ],
   "source": [
    "retriever = db_connection.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "print(type(retriever))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71b6cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = 'Explain this Resume'\n",
    "retrieved_docs = retriever.invoke(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e876b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADDITIONAL INFORMATION \n",
      "● Technical Skills: Python,Flask,Streamlit,SQL&DBMS,EDA,MLOPS,Machine Learning,Data \n",
      "Analytics,PowerBI, NLP, AWS Deployment.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa62d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    # System Message Prompt Template\n",
    "    SystemMessage(content=\"\"\"You are a Helpful AI Bot. \n",
    "    You take the context and question from user. Your answer should be based on the specific context.\"\"\"),\n",
    "    # Human Message Prompt Template\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"Aswer the question based on the given context.\n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: \n",
    "    {question}\n",
    "    \n",
    "    Answer: \"\"\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ce6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7904619",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f4c3e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "chat_model = ChatGoogleGenerativeAI(google_api_key=\"YOUR_API_KEY\", \n",
    "                                   model=\"gemini-1.5-pro-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ab5fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04c0ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | chat_template\n",
    "    | chat_model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45b195a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Potential Job Roles Based on Your Skills:\\n\\nGiven your technical skills in Python, Flask, Streamlit, SQL & DBMS, EDA, MLOPS, Machine Learning, Data Analytics, PowerBI, NLP, and AWS Deployment, you have a strong foundation for several exciting job roles:\\n\\n**Data Science & Machine Learning:**\\n\\n* **Data Scientist:** Analyze large datasets, build and deploy machine learning models, and derive insights to solve business problems.\\n* **Machine Learning Engineer:** Focus on the development and deployment of machine learning models, including model optimization and scaling.\\n* **NLP Engineer:** Develop natural language processing applications, such as chatbots, text summarization tools, and sentiment analysis systems.\\n\\n**Data Analytics & Business Intelligence:**\\n\\n* **Data Analyst:** Collect, clean, and analyze data to identify trends, patterns, and insights, and create reports and visualizations to communicate findings.\\n* **Business Intelligence Analyst:** Develop and maintain BI dashboards and reports, using tools like PowerBI, to track key performance indicators and support data-driven decision making.\\n\\n**Software Development & MLOps:**\\n\\n* **Backend Developer:** Build and maintain the server-side logic of web applications using Python and Flask.\\n* **MLOps Engineer:** Design and implement MLOps pipelines to automate the machine learning model lifecycle, ensuring efficient and reliable model deployment.\\n\\n**Additional Potential Roles:**\\n\\n* **Research Scientist:** Conduct research in areas like machine learning, NLP, or artificial intelligence, and contribute to the advancement of the field.\\n* **Data Engineer:** Design, build, and maintain data infrastructure, including data pipelines and databases, to support data-driven applications.\\n\\n**Choosing the Right Path:**\\n\\n* **Interests:** Consider which areas within data science, machine learning, and analytics you find most engaging.\\n* **Experience:** Leverage your existing experience and skills to identify roles that best align with your background.\\n* **Industry:** Explore different industries, such as technology, finance, healthcare, or e-commerce, to find one that interests you.\\n\\n**Further Development:**\\n\\n* **Stay updated:** Continuously learn and adapt to new technologies and trends in the field.\\n* **Build a portfolio:** Showcase your skills and projects through a portfolio website or GitHub repository.\\n* **Network:** Connect with professionals in your desired field to learn about job opportunities and industry insights.\\n\\nI hope this information helps! Let me know if you have any further questions. \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"Can you tell me about job?\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91514611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Potential Job Roles Based on Your Skills:\n",
       "\n",
       "Given your technical skills in Python, Flask, Streamlit, SQL & DBMS, EDA, MLOPS, Machine Learning, Data Analytics, PowerBI, NLP, and AWS Deployment, you have a strong foundation for several exciting job roles:\n",
       "\n",
       "**Data Science & Machine Learning:**\n",
       "\n",
       "* **Data Scientist:** Analyze large datasets, build and deploy machine learning models, and derive insights to solve business problems.\n",
       "* **Machine Learning Engineer:** Focus on the development and deployment of machine learning models, including model optimization and scaling.\n",
       "* **NLP Engineer:** Develop natural language processing applications, such as chatbots, text summarization tools, and sentiment analysis systems.\n",
       "\n",
       "**Data Analytics & Business Intelligence:**\n",
       "\n",
       "* **Data Analyst:** Collect, clean, and analyze data to identify trends, patterns, and insights, and create reports and visualizations to communicate findings.\n",
       "* **Business Intelligence Analyst:** Develop and maintain BI dashboards and reports, using tools like PowerBI, to track key performance indicators and support data-driven decision making.\n",
       "\n",
       "**Software Development & MLOps:**\n",
       "\n",
       "* **Backend Developer:** Build and maintain the server-side logic of web applications using Python and Flask.\n",
       "* **MLOps Engineer:** Design and implement MLOps pipelines to automate the machine learning model lifecycle, ensuring efficient and reliable model deployment.\n",
       "\n",
       "**Additional Potential Roles:**\n",
       "\n",
       "* **Research Scientist:** Conduct research in areas like machine learning, NLP, or artificial intelligence, and contribute to the advancement of the field.\n",
       "* **Data Engineer:** Design, build, and maintain data infrastructure, including data pipelines and databases, to support data-driven applications.\n",
       "\n",
       "**Choosing the Right Path:**\n",
       "\n",
       "* **Interests:** Consider which areas within data science, machine learning, and analytics you find most engaging.\n",
       "* **Experience:** Leverage your existing experience and skills to identify roles that best align with your background.\n",
       "* **Industry:** Explore different industries, such as technology, finance, healthcare, or e-commerce, to find one that interests you.\n",
       "\n",
       "**Further Development:**\n",
       "\n",
       "* **Stay updated:** Continuously learn and adapt to new technologies and trends in the field.\n",
       "* **Build a portfolio:** Showcase your skills and projects through a portfolio website or GitHub repository.\n",
       "* **Network:** Connect with professionals in your desired field to learn about job opportunities and industry insights.\n",
       "\n",
       "I hope this information helps! Let me know if you have any further questions. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361361ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
